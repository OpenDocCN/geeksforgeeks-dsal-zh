# 彩票假说

> 原文:[https://www . geesforgeks . org/the-彩票-假设/](https://www.geeksforgeeks.org/the-lottery-ticket-hypothesis/)

麻省理工学院-国际商用机器公司沃森人工智能实验室在 2019 年 ICLR 以研究论文的形式提出了彩票假说。该论文获得了 2019 年 ICLR 最佳论文奖。

**背景:网络修剪**
**修剪**基本上是指通过去除多余和不需要的部分来减少神经网络的范围。网络修剪是一种常用的实践，用于减少神经网络的大小、存储和计算空间。比如——在你的手机里安装一个完整的神经网络。网络修剪的思想起源于 20 世纪 90 年代，后来在 2015 年得到普及。

**如何“修剪”一个神经网络？**
我们可以把修剪的过程总结为 4 个主要步骤:

1.  训练网络
2.  移除多余的结构
3.  微调网络
4.  可选:重复步骤 2 和 3

但是，在我们进一步前进之前，你必须知道:

*   通常，修剪是在对数据训练神经网络后进行的。
*   多余的结构可以是*权重、神经元、过滤器、通道。*但是，这里我们考虑“稀疏修剪”，意思是修剪“权重”。
*   需要一个*启发式*来定义一个结构是否多余。这些试探法是*幅度、梯度或激活。*这里我们选择了*星等*。我们用最低的幅度修剪权重。
*   通过去除神经网络中的部分，我们多少破坏了激活功能。因此，我们对模型进行了更多的训练。这就是所谓的**微调**。

    *9x to 12x*

    **Can’t we randomly initialize a pruned network and train to convergence?**

    *Training a pruned model from scratch performs worse than retraining a pruned model, which may indicate the difficulty of training a network with small capacity* 

    **How to train pruned networks ?**

1.  随机初始化整个网络
2.  训练它，删除多余的结构
3.  步骤 1 后，将每个剩余重量重置为其值。

这基本上表明“在随机初始化的深度神经网络中存在一个子网，当孤立地训练时，它可以匹配甚至超过原始网络的精度。

**训练有素的修剪网络的优势**

*   像 MNIST 这样具有超过 *600K 个参数*的全连接神经网络被认为被简化为具有与原始网络相同精度的 21K 个参数的*子网*
*   保留原有的功能–丢失、重量衰减、批处理、resnet、您最喜欢的优化器等。

**进一步研究范围**

*   子网被追溯发现
*   寻找子网非常昂贵
*   小型视觉网络和任务

链接到研究论文:[彩票假说:寻找稀疏的、可训练的神经网络](https://openreview.net/pdf?id=rJl-b3RcF7)